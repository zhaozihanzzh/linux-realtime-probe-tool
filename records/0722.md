# 第 8 周

## 选题解读

在真正考虑如何实现功能时，我原本设想的是在内核模块中维护当前持有的所有锁的列表，当有锁被获取、释放时都需要进行记录，这样就能输出进程持有的锁，但 7 月 22 日与马玉昆老师会议交流时，马老师认为只需要记录获取锁，不需要在意释放锁的动作，且我们需要在意哪些进程和关中断的进程持有相同的锁，以达到更好的分析效果。

## kernel 学习

### 自旋锁、读写锁

自旋锁用于需要短时间加锁，且不希望进行进程调度（即，请求锁的进程不会睡眠）时的场景。一般而言，在自旋时，中断是被关闭的，否则中断处理程序可能会打断当前正在临界区中执行的线程，这时中断处理程序申请同一把自旋锁时会发生死锁。可见，自旋锁与内核的实时性是存在关联的。

以下是与自旋锁相关的重要 API：

|  获得  |  释放  |
|  ----  |  ----  |
| spin_lock  | spin_unlock |
| spin_lock_bh | spin_unlock_bh |
| spin_lock_irq | spin_unlock_irq |
| spin_lock_irqsave  | spin_unlock_irqrestore |

除了加锁解锁外，`spin_lock_bh` 会禁止下半部，而 `spin_unlock_bh` 会使能下半部；`spin_lock_irq` 会关本地中断，而 `spin_unlock_irq` 会开本地中断；`spin_lock_irqsave` 会先保存本地中断的状态，再禁止本地中断，而 `spin_unlock_irqrestore` 会让本地中断恢复到已保存的状态。若尝试获得某个自旋锁时该锁已被占用，则将持续地原地等待、不停询问，直到竞争成功得到该锁。若希望在锁已被占用时直接返回非 0 值，可以使用名字带有 trylock 的函数。

- spin_trylock
- spin_trylock_bh
- spin_trylock_irq
- spin_trylock_irqsave

此外，还有一个 spin_lock_nested，在不开编译选项 `CONFIG_DEBUG_LOCK_ALLOC` 时应该最后会调用 `_raw_spin_lock`，这个似乎是用来死锁检测的。

如果有明确的读写划分，也可以使用读-写自旋锁，可以并发持有读锁，但只有一个进程能获得写锁，且获取写锁时不可以有读操作。读锁相关 API、写锁相关 API 与 spin_lock 系列命名类似，但以 read_lock、write_lock 开头。

由于我们计划使用 KProbe，有必要了解背后的相关接口。我猜想可能是为了保持开关 lockdep 等功能时对外的接口一致，大多数函数或宏调用了以 raw 开头的内联函数或宏，以 raw 开头的内联函数或宏又调用了以 _raw 开头的函数：

kernel/locking/spinlock.c
```c
#ifndef CONFIG_INLINE_SPIN_TRYLOCK_BH
int __lockfunc _raw_spin_trylock_bh(raw_spinlock_t *lock)
{
	return __raw_spin_trylock_bh(lock);
}
EXPORT_SYMBOL(_raw_spin_trylock_bh);
#endif

#ifndef CONFIG_INLINE_SPIN_LOCK
void __lockfunc _raw_spin_lock(raw_spinlock_t *lock)
{
	__raw_spin_lock(lock);
}
EXPORT_SYMBOL(_raw_spin_lock);
#endif

#ifndef CONFIG_INLINE_SPIN_LOCK_IRQSAVE
unsigned long __lockfunc _raw_spin_lock_irqsave(raw_spinlock_t *lock)
{
	return __raw_spin_lock_irqsave(lock);
}
EXPORT_SYMBOL(_raw_spin_lock_irqsave);
#endif

#ifndef CONFIG_INLINE_SPIN_LOCK_IRQ
void __lockfunc _raw_spin_lock_irq(raw_spinlock_t *lock)
{
	__raw_spin_lock_irq(lock);
}
EXPORT_SYMBOL(_raw_spin_lock_irq);
#endif

#ifndef CONFIG_INLINE_SPIN_LOCK_BH
void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock)
{
	__raw_spin_lock_bh(lock);
}
EXPORT_SYMBOL(_raw_spin_lock_bh);
#endif

#ifdef CONFIG_UNINLINE_SPIN_UNLOCK
void __lockfunc _raw_spin_unlock(raw_spinlock_t *lock)
{
	__raw_spin_unlock(lock);
}
EXPORT_SYMBOL(_raw_spin_unlock);
#endif

#ifndef CONFIG_INLINE_SPIN_UNLOCK_IRQRESTORE
void __lockfunc _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
{
	__raw_spin_unlock_irqrestore(lock, flags);
}
EXPORT_SYMBOL(_raw_spin_unlock_irqrestore);
#endif

#ifndef CONFIG_INLINE_SPIN_UNLOCK_IRQ
void __lockfunc _raw_spin_unlock_irq(raw_spinlock_t *lock)
{
	__raw_spin_unlock_irq(lock);
}
EXPORT_SYMBOL(_raw_spin_unlock_irq);
#endif

#ifndef CONFIG_INLINE_SPIN_UNLOCK_BH
void __lockfunc _raw_spin_unlock_bh(raw_spinlock_t *lock)
{
	__raw_spin_unlock_bh(lock);
}
EXPORT_SYMBOL(_raw_spin_unlock_bh);
#endif

#ifndef CONFIG_INLINE_READ_TRYLOCK
int __lockfunc _raw_read_trylock(rwlock_t *lock)
{
	return __raw_read_trylock(lock);
}
EXPORT_SYMBOL(_raw_read_trylock);
#endif

#ifndef CONFIG_INLINE_READ_LOCK
void __lockfunc _raw_read_lock(rwlock_t *lock)
{
	__raw_read_lock(lock);
}
EXPORT_SYMBOL(_raw_read_lock);
#endif

#ifndef CONFIG_INLINE_READ_LOCK_IRQSAVE
unsigned long __lockfunc _raw_read_lock_irqsave(rwlock_t *lock)
{
	return __raw_read_lock_irqsave(lock);
}
EXPORT_SYMBOL(_raw_read_lock_irqsave);
#endif

#ifndef CONFIG_INLINE_READ_LOCK_IRQ
void __lockfunc _raw_read_lock_irq(rwlock_t *lock)
{
	__raw_read_lock_irq(lock);
}
EXPORT_SYMBOL(_raw_read_lock_irq);
#endif

#ifndef CONFIG_INLINE_READ_LOCK_BH
void __lockfunc _raw_read_lock_bh(rwlock_t *lock)
{
	__raw_read_lock_bh(lock);
}
EXPORT_SYMBOL(_raw_read_lock_bh);
#endif

#ifndef CONFIG_INLINE_READ_UNLOCK
void __lockfunc _raw_read_unlock(rwlock_t *lock)
{
	__raw_read_unlock(lock);
}
EXPORT_SYMBOL(_raw_read_unlock);
#endif

#ifndef CONFIG_INLINE_READ_UNLOCK_IRQRESTORE
void __lockfunc _raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
{
	__raw_read_unlock_irqrestore(lock, flags);
}
EXPORT_SYMBOL(_raw_read_unlock_irqrestore);
#endif

#ifndef CONFIG_INLINE_READ_UNLOCK_IRQ
void __lockfunc _raw_read_unlock_irq(rwlock_t *lock)
{
	__raw_read_unlock_irq(lock);
}
EXPORT_SYMBOL(_raw_read_unlock_irq);
#endif

#ifndef CONFIG_INLINE_READ_UNLOCK_BH
void __lockfunc _raw_read_unlock_bh(rwlock_t *lock)
{
	__raw_read_unlock_bh(lock);
}
EXPORT_SYMBOL(_raw_read_unlock_bh);
#endif

#ifndef CONFIG_INLINE_WRITE_TRYLOCK
int __lockfunc _raw_write_trylock(rwlock_t *lock)
{
	return __raw_write_trylock(lock);
}
EXPORT_SYMBOL(_raw_write_trylock);
#endif

#ifndef CONFIG_INLINE_WRITE_LOCK
void __lockfunc _raw_write_lock(rwlock_t *lock)
{
	__raw_write_lock(lock);
}
EXPORT_SYMBOL(_raw_write_lock);
#endif

#ifndef CONFIG_INLINE_WRITE_LOCK_IRQSAVE
unsigned long __lockfunc _raw_write_lock_irqsave(rwlock_t *lock)
{
	return __raw_write_lock_irqsave(lock);
}
EXPORT_SYMBOL(_raw_write_lock_irqsave);
#endif

#ifndef CONFIG_INLINE_WRITE_LOCK_IRQ
void __lockfunc _raw_write_lock_irq(rwlock_t *lock)
{
	__raw_write_lock_irq(lock);
}
EXPORT_SYMBOL(_raw_write_lock_irq);
#endif

#ifndef CONFIG_INLINE_WRITE_LOCK_BH
void __lockfunc _raw_write_lock_bh(rwlock_t *lock)
{
	__raw_write_lock_bh(lock);
}
EXPORT_SYMBOL(_raw_write_lock_bh);
#endif

#ifndef CONFIG_INLINE_WRITE_UNLOCK
void __lockfunc _raw_write_unlock(rwlock_t *lock)
{
	__raw_write_unlock(lock);
}
EXPORT_SYMBOL(_raw_write_unlock);
#endif

#ifndef CONFIG_INLINE_WRITE_UNLOCK_IRQRESTORE
void __lockfunc _raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
{
	__raw_write_unlock_irqrestore(lock, flags);
}
EXPORT_SYMBOL(_raw_write_unlock_irqrestore);
#endif

#ifndef CONFIG_INLINE_WRITE_UNLOCK_IRQ
void __lockfunc _raw_write_unlock_irq(rwlock_t *lock)
{
	__raw_write_unlock_irq(lock);
}
EXPORT_SYMBOL(_raw_write_unlock_irq);
#endif

#ifndef CONFIG_INLINE_WRITE_UNLOCK_BH
void __lockfunc _raw_write_unlock_bh(rwlock_t *lock)
{
	__raw_write_unlock_bh(lock);
}
EXPORT_SYMBOL(_raw_write_unlock_bh);
#endif
```

经过测试，用 KProbe 追踪 _raw 开头的函数基本不需要调整内核编译选项，是较为合适的。其他的函数往往由于内联等原因无法被追踪。由于 KProbe 在回调函数执行时[不会触发嵌套的追踪](0522.md#kprobe)，我们仍然可以在回调函数中使用自旋锁。为了验证马老师的观点，即所有锁最后都会调用共同的函数（应该指的是 `_raw_spin_lock`），因此只需要一个 KProbe，我们使用[自编译内核](0515.md#自编译内核引导-centos-启动)进行[调试](0511.md#调试内核)，得到如下的结果：

### spin_lock

```text
(gdb) hbreak _raw_spin_lock
Hardware assisted breakpoint 4 at 0xffffffff819ac083: file kernel/locking/spinlock.c, line 150.
(gdb) c
Continuing.

Thread 1 hit Breakpoint 4, _raw_spin_lock (
    lock=0xffffffff82807a40 <jiffies_lock>) at kernel/locking/spinlock.c:150
150	{
(gdb) s
151		__raw_spin_lock(lock);
(gdb) s
__raw_spin_lock (lock=0xffffffff82807a40 <jiffies_lock>)
    at kernel/locking/spinlock.c:151
151		__raw_spin_lock(lock);
(gdb) s
do_raw_spin_lock (lock=0xffffffff82807a40 <jiffies_lock>)
    at kernel/locking/spinlock.c:151
151		__raw_spin_lock(lock);
(gdb) s
queued_spin_lock (lock=0xffffffff82807a40 <jiffies_lock>)
    at kernel/locking/spinlock.c:151
151		__raw_spin_lock(lock);
(gdb) s
atomic_cmpxchg (new=1, old=0, v=0xffffffff82807a40 <jiffies_lock>)
    at kernel/locking/spinlock.c:151
151		__raw_spin_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=1, old=0, v=0xffffffff82807a40 <jiffies_lock>)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
queued_spin_lock (lock=0xffffffff82807a40 <jiffies_lock>)
    at ./include/asm-generic/qspinlock.h:87
87		if (likely(val == 0))
(gdb) s
tick_do_update_jiffies64 (now=now@entry=65247750746)
    at kernel/time/tick-sched.c:99
99		if (ktime_before(now, tick_next_period)) {
```

### read_lock

```c
#if !defined(CONFIG_GENERIC_LOCKBREAK) || defined(CONFIG_DEBUG_LOCK_ALLOC)

static inline void __raw_read_lock(rwlock_t *lock)
{
        preempt_disable();
        rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
        LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);
}
```

```text
(gdb) hbreak _raw_read_lock
Hardware assisted breakpoint 5 at 0xffffffff819abefe: file kernel/locking/spinlock.c, line 222.
(gdb) delete breakpoints 4
(gdb) c
Continuing.

Thread 2 hit Breakpoint 5, _raw_read_lock (
    lock=0xffffffff8331d398 <selinux_ss+600>) at kernel/locking/spinlock.c:222
222	{
(gdb) s
223		__raw_read_lock(lock);
(gdb) s
__raw_read_lock (lock=0xffffffff8331d398 <selinux_ss+600>)
    at kernel/locking/spinlock.c:223
223		__raw_read_lock(lock);
(gdb) s
queued_read_lock (lock=0xffffffff8331d398 <selinux_ss+600>)
    at kernel/locking/spinlock.c:223
223		__raw_read_lock(lock);
(gdb) s
atomic_add_return (v=0xffffffff8331d398 <selinux_ss+600>, i=512)
    at kernel/locking/spinlock.c:223
223		__raw_read_lock(lock);
(gdb) s
arch_atomic_add_return (v=0xffffffff8331d398 <selinux_ss+600>, i=512)
    at ./arch/x86/include/asm/atomic.h:165
165		return i + xadd(&v->counter, i);
(gdb) s
queued_read_lock (lock=0xffffffff8331d398 <selinux_ss+600>)
    at ./include/asm-generic/qrwlock.h:88
88		if (likely(!(cnts & _QW_WMASK)))
(gdb) s
security_compute_sid (state=0xffffffff83319c80 <selinux_state>, ssid=1, 
    tsid=28, orig_tclass=orig_tclass@entry=7, specified=specified@entry=16, 
    objname=0x0 <fixed_percpu_data>, out_sid=0xffffc90000bcfb94, kern=true)
    at security/selinux/ss/services.c:1720
1720		if (kern) {
```

### write_lock

```text
(gdb) hbreak _raw_write_lock
Hardware assisted breakpoint 6 at 0xffffffff819abe93: file kernel/locking/spinlock.c, line 294.
(gdb) delete breakpoints 5
(gdb) c
Continuing.

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
warning: Error removing breakpoint 0

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
warning: Error removing breakpoint 0

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
warning: Error removing breakpoint 0

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
warning: Error removing breakpoint 0

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
warning: Error removing breakpoint 0

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
warning: Error removing breakpoint 0

Thread 2 hit Breakpoint 6, _raw_write_lock (lock=0xffff888108869228)
    at kernel/locking/spinlock.c:294
294	{
(gdb) s
295		__raw_write_lock(lock);
(gdb) s
__raw_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
queued_write_lock (lock=0xffff888108869228) at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at kernel/locking/spinlock.c:295
295		__raw_write_lock(lock);
(gdb) s
arch_atomic_cmpxchg (new=255, old=0, v=0xffff888108869228)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
^Cwarning: Error removing breakpoint 0

Thread 2 received signal SIGINT, Interrupt.
csd_lock_wait (csd=0xffff88813bc30940) at kernel/smp.c:108
108		smp_cond_load_acquire(&csd->flags, !(VAL & CSD_FLAG_LOCK));
(gdb) 
```

### semaphore

可以看到，此时调用了 `_raw_spin_lock_irqsave`：
```text
(gdb) hbreak down
Hardware assisted breakpoint 7 at 0xffffffff81137745: file kernel/locking/semaphore.c, line 55.
(gdb) delete breakpoints 6
(gdb) c
Continuing.
[Switching to Thread 1.1]

Thread 1 hit Breakpoint 7, down (sem=0xffff8881088428d0)
    at kernel/locking/semaphore.c:55
55	{
(gdb) s
58		raw_spin_lock_irqsave(&sem->lock, flags);
(gdb) s
_raw_spin_lock_irqsave (lock=lock@entry=0xffff8881088428d0)
    at kernel/locking/spinlock.c:158
158	{
(gdb) s
159		return __raw_spin_lock_irqsave(lock);
(gdb) s
__raw_spin_lock_irqsave (lock=0xffff8881088428d0)
    at kernel/locking/spinlock.c:159
159		return __raw_spin_lock_irqsave(lock);
(gdb) s
arch_local_irq_save () at kernel/locking/spinlock.c:159
159		return __raw_spin_lock_irqsave(lock);
(gdb) s
arch_local_save_flags () at ./arch/x86/include/asm/paravirt.h:811
811		f = arch_local_save_flags();
(gdb) s
arch_local_irq_save () at ./arch/x86/include/asm/paravirt.h:812
812		arch_local_irq_disable();
(gdb) s
arch_local_irq_disable () at ./arch/x86/include/asm/paravirt.h:812
812		arch_local_irq_disable();
(gdb) s
__raw_spin_lock_irqsave (lock=0xffff8881088428d0)
    at ./arch/x86/include/asm/irqflags.h:165
165		return !(flags & X86_EFLAGS_IF);
(gdb) s
_raw_spin_lock_irqsave (lock=lock@entry=0xffff8881088428d0)
    at kernel/locking/spinlock.c:159
159		return __raw_spin_lock_irqsave(lock);
(gdb) s
__raw_spin_lock_irqsave (lock=0xffff8881088428d0)
    at ./include/linux/spinlock_api_smp.h:108
108		local_irq_save(flags);
(gdb) s
trace_hardirqs_off () at kernel/trace/trace_preemptirq.c:75
75	{
(gdb) s
78		if (!this_cpu_read(tracing_irq_cpu)) {
(gdb) s
79			this_cpu_write(tracing_irq_cpu, 1);
(gdb) s
81			if (!in_nmi())
(gdb) s
preempt_count () at ./arch/x86/include/asm/preempt.h:26
26		return raw_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED;
(gdb) s
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
trace_irq_disable_rcuidle (parent_ip=0, ip=18446744071588986849)
    at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
static_key_false (key=0xffffffff829e8688 <__tracepoint_irq_disable+8>)
    at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
arch_static_branch (branch=false, 
    key=0xffffffff829e8688 <__tracepoint_irq_disable+8>)
    at ./arch/x86/include/asm/jump_label.h:38
38		asm_volatile_goto("1:"
(gdb) s
trace_hardirqs_off () at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
trace_irq_disable_rcuidle (parent_ip=0, ip=18446744071588986849)
    at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
static_key_false (key=<optimized out>) at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
arch_static_branch (branch=<optimized out>, key=<optimized out>)
    at ./arch/x86/include/asm/jump_label.h:46
46		return false;
(gdb) s
__raw_spin_lock_irqsave (lock=0xffff8881088428d0)
    at ./include/linux/spinlock_api_smp.h:119
119		do_raw_spin_lock_flags(lock, &flags);
(gdb) s
do_raw_spin_lock_flags (flags=<synthetic pointer>, lock=0xffff8881088428d0)
    at ./include/linux/spinlock_api_smp.h:119
119		do_raw_spin_lock_flags(lock, &flags);
(gdb) s
queued_spin_lock (lock=0xffff8881088428d0)
    at ./include/linux/spinlock_api_smp.h:119
119		do_raw_spin_lock_flags(lock, &flags);
(gdb) s
atomic_cmpxchg (new=1, old=0, v=0xffff8881088428d0)
    at ./include/linux/spinlock_api_smp.h:119
119		do_raw_spin_lock_flags(lock, &flags);
(gdb) s
arch_atomic_cmpxchg (new=1, old=0, v=0xffff8881088428d0)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
queued_spin_lock (lock=0xffff8881088428d0)
    at ./include/asm-generic/qspinlock.h:87
87		if (likely(val == 0))
(gdb) s
down (sem=0xffff8881088428d0) at kernel/locking/semaphore.c:59
59		if (likely(sem->count > 0))
(gdb) s
60			sem->count--;
(gdb) s
63		raw_spin_unlock_irqrestore(&sem->lock, flags);
```
目前暂未验证其他执行路径下是否也会调用 `_raw_spin_lock_irqsave`。

### mutex

```c
void __sched mutex_lock(struct mutex *lock)
{
        might_sleep();

        if (!__mutex_trylock_fast(lock))
                __mutex_lock_slowpath(lock);
}
EXPORT_SYMBOL(mutex_lock);
```
可以看到，此时调用了 `_raw_spin_lock_irqsave`：
```text
(gdb) hbreak mutex_lock
Hardware assisted breakpoint 11 at 0xffffffff819a7ed7: file kernel/locking/mutex.c, line 279.
(gdb) c
Continuing.
[Switching to Thread 1.2]

Thread 2 hit Breakpoint 11, mutex_lock (lock=lock@entry=0xffff88810a0c43c8)
    at kernel/locking/mutex.c:279
279	{
(gdb) s
280		might_sleep();
(gdb) n
282		if (!__mutex_trylock_fast(lock))
(gdb) s
__mutex_trylock_fast (lock=0xffff88810a0c43c8) at kernel/locking/mutex.c:282
282		if (!__mutex_trylock_fast(lock))
(gdb) s
get_current () at ./arch/x86/include/asm/current.h:15
15		return this_cpu_read_stable(current_task);
(gdb) s
__mutex_trylock_fast (lock=0xffff88810a0c43c8) at kernel/locking/mutex.c:170
170		if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))
(gdb) s
atomic64_try_cmpxchg (new=-131387241543680, old=<synthetic pointer>, 
    v=0xffff88810a0c43c8) at kernel/locking/mutex.c:170
170		if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))
(gdb) s
arch_atomic64_try_cmpxchg (new=-131387241543680, old=<synthetic pointer>, 
    v=0xffff88810a0c43c8) at ./arch/x86/include/asm/atomic64_64.h:186
186		return try_cmpxchg(&v->counter, old, new);
(gdb) s
ep_scan_ready_list (ep=ep@entry=0xffff88810a0c43c0, 
    sproc=sproc@entry=0xffffffff81366c9d <ep_send_events_proc>, 
    priv=priv@entry=0xffffc90000b43e20, depth=depth@entry=0, 
    ep_locked=ep_locked@entry=false) at ./include/linux/spinlock.h:329
329		return &lock->rlock;
(gdb) s
_raw_spin_lock_irqsave (lock=lock@entry=0xffff88810a0c43c0)
    at kernel/locking/spinlock.c:158
158	{
(gdb) s
159		return __raw_spin_lock_irqsave(lock);
(gdb) s
__raw_spin_lock_irqsave (lock=0xffff88810a0c43c0)
    at kernel/locking/spinlock.c:159
159		return __raw_spin_lock_irqsave(lock);
(gdb) s
arch_local_irq_save () at kernel/locking/spinlock.c:159
159		return __raw_spin_lock_irqsave(lock);
(gdb) s
arch_local_save_flags () at ./arch/x86/include/asm/paravirt.h:811
811		f = arch_local_save_flags();
(gdb) s
arch_local_irq_save () at ./arch/x86/include/asm/paravirt.h:812
812		arch_local_irq_disable();
(gdb) s
arch_local_irq_disable () at ./arch/x86/include/asm/paravirt.h:812
812		arch_local_irq_disable();
(gdb) s
__raw_spin_lock_irqsave (lock=0xffff88810a0c43c0)
    at ./arch/x86/include/asm/irqflags.h:165
165		return !(flags & X86_EFLAGS_IF);
(gdb) s
_raw_spin_lock_irqsave (lock=lock@entry=0xffff88810a0c43c0)
    at kernel/locking/spinlock.c:159
159		return __raw_spin_lock_irqsave(lock);
(gdb) s
__raw_spin_lock_irqsave (lock=0xffff88810a0c43c0)
    at ./include/linux/spinlock_api_smp.h:108
108		local_irq_save(flags);
(gdb) s
trace_hardirqs_off () at kernel/trace/trace_preemptirq.c:75
75	{
(gdb) s
78		if (!this_cpu_read(tracing_irq_cpu)) {
(gdb) s
79			this_cpu_write(tracing_irq_cpu, 1);
(gdb) s
81			if (!in_nmi())
(gdb) s
preempt_count () at ./arch/x86/include/asm/preempt.h:26
26		return raw_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED;
(gdb) s
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
trace_irq_disable_rcuidle (parent_ip=0, ip=18446744071588986849)
    at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
static_key_false (key=0xffffffff829e8688 <__tracepoint_irq_disable+8>)
    at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
arch_static_branch (branch=false, 
    key=0xffffffff829e8688 <__tracepoint_irq_disable+8>)
    at ./arch/x86/include/asm/jump_label.h:38
38		asm_volatile_goto("1:"
(gdb) s
trace_hardirqs_off () at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
trace_irq_disable_rcuidle (parent_ip=0, ip=18446744071588986849)
    at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
static_key_false (key=<optimized out>) at kernel/trace/trace_preemptirq.c:82
82				trace_irq_disable_rcuidle(CALLER_ADDR0, CALLER_ADDR1);
(gdb) s
arch_static_branch (branch=<optimized out>, key=<optimized out>)
    at ./arch/x86/include/asm/jump_label.h:46
46		return false;
(gdb) s
__raw_spin_lock_irqsave (lock=0xffff88810a0c43c0)
    at ./include/linux/spinlock_api_smp.h:119
119		do_raw_spin_lock_flags(lock, &flags);
(gdb) s
do_raw_spin_lock_flags (flags=<synthetic pointer>, lock=0xffff88810a0c43c0)
    at ./include/linux/spinlock_api_smp.h:119
119		do_raw_spin_lock_flags(lock, &flags);
(gdb) s
queued_spin_lock (lock=0xffff88810a0c43c0)
    at ./include/linux/spinlock_api_smp.h:119
119		do_raw_spin_lock_flags(lock, &flags);
(gdb) s
atomic_cmpxchg (new=1, old=0, v=0xffff88810a0c43c0)
    at ./include/linux/spinlock_api_smp.h:119
119		do_raw_spin_lock_flags(lock, &flags);
(gdb) s
arch_atomic_cmpxchg (new=1, old=0, v=0xffff88810a0c43c0)
    at ./arch/x86/include/asm/atomic.h:192
192		return arch_cmpxchg(&v->counter, old, new);
(gdb) s
queued_spin_lock (lock=0xffff88810a0c43c0)
    at ./include/asm-generic/qspinlock.h:87
87		if (likely(val == 0))
(gdb) s
ep_scan_ready_list (ep=ep@entry=0xffff88810a0c43c0, 
    sproc=sproc@entry=0xffffffff81366c9d <ep_send_events_proc>, 
    priv=priv@entry=0xffffc90000b43e20, depth=depth@entry=0, 
    ep_locked=ep_locked@entry=false) at fs/eventpoll.c:696
696		list_splice_init(&ep->rdllist, &txlist);
```
我们看到以上代码走的是 `__mutex_trylock_fast` 这一支：
```c
/*
 * Optimistic trylock that only works in the uncontended case. Make sure to
 * follow with a __mutex_trylock() before failing.
 */
static __always_inline bool __mutex_trylock_fast(struct mutex *lock)
{
	unsigned long curr = (unsigned long)current;
	unsigned long zero = 0UL;

	if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr))
		return true;

	return false;
}
```
对于另一个分支，调用了 `__mutex_lock_slowpath`：
```c
static noinline void __sched
__mutex_lock_slowpath(struct mutex *lock)
{
	__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);
}
```
遗憾的是，暂时还没查明这一支是否也会调用 `_raw_spin_lock_irqsave`。

可以猜测，信号量等的函数执行时应当调用了自旋锁的函数，但读-写自旋锁用到的函数和自旋锁的并不相同，可能只记录 `raw_spin_lock` 并不行，我们已经与马老师取得了进一步交流，在此期间我们先考虑如何记录不同进程持有的锁的地址等信息。

## 内核模块开发

### IDR 建立映射关系

Linux 曾经使用过 pidhash，但这种借助哈希表的方式似乎后来[被移除了](https://www.mail-archive.com/linux-kernel@vger.kernel.org/msg1508232.html)，我们可以使用 [IDR 机制](https://www.kernel.org/doc/html/v4.18/core-api/idr.html)，将整型值借助基数树与指针相关联。

IDR 用 `DEFINE_IDR()` 静态分配，用 `idr_alloc()` 分配一个未使用的 ID，用 `idr_find()` 取得 ID 关联的指针，用 `idr_remove()` 清除 ID，用 `idr_replace()` 替换 ID 关联的指针。

默认的 ID 大小不超过 `INT_MAX`，如果要求顺序分配 ID 的话，可以使用 `idr_alloc_cyclic()`，但处理更大的 ID 时 IDR 效率会降低。遍历时可以使用 `idr_for_each()` 或 `idr_for_each_entry()`，也可以用 `idr_get_next()` 获取下一项。

可以调用 `idr_destroy()` 来释放 IDR，但这并不会释放指针指向的对象。用 `idr_is_empty()` 判断当前是否有分配的 ID。

得益于 RCU，`idr_find()` 可以被无锁地调用，调用者必须确保调用时是在 `rcu_read_lock()` 区域中。

看起来 IDR 不错，然而我们希望用锁的地址作为查找的索引，而不是 ID，可能不能使用这种数据结构。

### 哈希表

Linux 内核中提供了[哈希表](https://lwn.net/Articles/510202/)，其表的长度固定，表项是 hlist_head，key 相同的元素会被放入链表中，以解决冲突。内核提供了预设的散列函数。hlist 与内核链表有些相似，但也有不同，可以阅读[这篇文章](https://danielmaker.github.io/blog/linux/list_hlist_hashtable.html)。这可能较为合适，但哈希表的大小、装填因子、是否要把不同种类的锁放在同一个哈希表中等问题仍然有待研究。